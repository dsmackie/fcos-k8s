https://www.codetab.org/post/kubernetes-cluster-virtualbox/
https://github.com/kubernetes-sigs/kubespray/issues/8197
https://jebpages.com/2019/02/25/installing-kubeadm-on-fedora-coreos/
https://www.marcolancini.it/2021/blog-kubernetes-lab-baremetal/
https://typhoon.psdn.io/fedora-coreos/bare-metal/
https://gist.github.com/codello/118ad0562bfd91e90d5914a9e2dbe606
https://www.codetab.org/post/kubernetes-cluster-virtualbox/
https://www.matthiaspreu.com/posts/fedora-coreos-kubernetes-basic-setup/

https://kubernetes.io/docs/setup/production-environment/container-runtimes/#cgroup-driver
https://github.com/coreos/fedora-coreos-tracker/issues/592

https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/#enable-shell-autocompletion


Personal access tokens

Tokens you have generated that can be used to access the GitHub API.
Make sure to copy your personal access token now. You won’t be able to see it again!
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
Personal access tokens function like ordinary OAuth access tokens. They can be used instead of a password for Git over HTTPS, or can be used to authenticate to the API over Basic Authentication. 

Personal access tokens

Tokens you have generated that can be used to access the GitHub API.
Make sure to copy your personal access token now. You won’t be able to see it again!
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
Personal access tokens function like ordinary OAuth access tokens. They can be used instead of a password for Git over HTTPS, or can be used to authenticate to the API over Basic Authentication. 



https://docs.ansible.com/ansible/latest/collections/ansible/posix/profile_tasks_callback.html

[p@ansible fcos-k8s]$ ansible-galaxy collection install ansible.posix
Starting galaxy collection install process
Process install dependency map
Starting collection install process
Downloading https://galaxy.ansible.com/download/ansible-posix-1.4.0.tar.gz to /home/p/.ansible/tmp/ansible-local-64018ecfbb4e/tmprrsk_inw/ansible-posix-1.4.0-syyloc98
Installing 'ansible.posix:1.4.0' to '/home/p/.ansible/collections/ansible_collections/ansible/posix'
ansible.posix:1.4.0 was installed successfully
[p@ansible fcos-k8s]$



00:A0:98:03:83:10	192.168.1.210
00:A0:98:03:83:4C	192.168.1.201
00:A0:98:03:83:4D	192.168.1.202
00:A0:98:03:83:4E	192.168.1.203
00:A0:98:03:83:4F	192.168.1.204


fedora-coreos-36.20220505.3.2-live.x86_64.iso

[p@ansible core]$ cat node1.fcc
variant: fcos
version: 1.4.0
passwd:
  users:
    - name: core
      groups:
        - docker
        - wheel
        - sudo
      ssh_authorized_keys:
        - ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAINHKcyMU729C1TDw+o+Ubh2Fz9VO11Ir6cOnVd30mKlG p@fedora
storage:
  files:
    - path: /etc/hostname
      overwrite: true
      contents:
        inline: node1.k8s.local
    - path: /etc/hosts
      overwrite: true
      contents:
        inline: |
          127.0.0.1       localhost             localhost.localdomain localhost4 localhost4.localdomain4
          ::1             localhost             localhost.localdomain localhost6 localhost6.localdomain6
          192.168.1.201   node1.k8s.local       node1
          192.168.1.202   node2.k8s.local       node2
          192.168.1.203   node3.k8s.local       node3
          192.168.1.204   node4.k8s.local       node4
          192.168.1.210   ansible.peter.local   ansible
    - path: /etc/sysctl.d/10-disable-ipv6.conf
      contents:
        inline: |
          net.ipv6.conf.all.disable_ipv6=1
          net.ipv6.conf.default.disable_ipv6=1
          net.ipv6.conf.enp0s4.disable_ipv6=1
systemd:
  units:
    - name: python3-for-ansible.service
      enabled: true
      contents: |
          [Unit]
          Requires=network-online.target
          After=network-online.target
          Before=sshd.service
          [Service]
          Type=oneshot
          ExecCondition=/usr/bin/test ! -f /etc/python3-for-ansible.done
          ExecStart=/usr/bin/sed -i '/\\[updates\\]/,/^\\[/ s/^enabled=.*$/enabled=0/' /etc/yum.repos.d/fedora-updates.repo
          ExecStart=/usr/bin/rpm-ostree install python3 libselinux-python3
          ExecStart=/usr/bin/sed -i '/\\[updates\\]/,/^\\[/ s/^enabled=.*$/enabled=1/' /etc/yum.repos.d/fedora-updates.repo
          ExecStart=/usr/bin/sed -i '/^\\[updates\\]/a exclude=libxcrypt-compat* mpdecimal* python-pip-wheel* python-setuptools-wheel* python-unversioned-command* python3* python3-libs* python3-selinux*' /etc/yum.repos.d/fedora-updates.repo
          ExecStartPost=/usr/bin/touch /etc/python3-for-ansible.done
          ExecStartPost=/usr/sbin/shutdown -r now
          [Install]
          WantedBy=multi-user.target






# installed packages for python
  libxcrypt-compat-4.4.28-1.fc36.x86_64
  mpdecimal-2.5.1-3.fc36.x86_64
  python-pip-wheel-21.3.1-2.fc36.noarch
  python-setuptools-wheel-59.6.0-2.fc36.noarch
  python-unversioned-command-3.10.4-1.fc36.noarch
  python3-3.10.4-1.fc36.x86_64
  python3-libs-3.10.4-1.fc36.x86_64
  python3-libselinux-3.3-4.fc36.x86_64


butane --pretty ./node1.fcc --output ./node1.ign
butane --pretty ./node2.fcc --output ./node2.ign
butane --pretty ./node3.fcc --output ./node3.ign
butane --pretty ./node4.fcc --output ./node4.ign


[p@ansible core]$ cat node1.ign
{
  "ignition": {
    "version": "3.3.0"
  },
  "passwd": {
    "users": [
      {
        "groups": [
          "docker",
          "wheel",
          "sudo"
        ],
        "name": "core",
        "sshAuthorizedKeys": [
          "ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAINHKcyMU729C1TDw+o+Ubh2Fz9VO11Ir6cOnVd30mKlG p@fedora"
        ]
      }
    ]
  },
  "storage": {
    "files": [
      {
        "overwrite": true,
        "path": "/etc/hostname",
        "contents": {
          "source": "data:,node1.k8s.local"
        }
      },
      {
        "overwrite": true,
        "path": "/etc/hosts",
        "contents": {
          "compression": "gzip",
          "source": "data:;base64,H4sIAAAAAAAC/5TN0arDIAzG8XufwicIJgZPT9/GrcLKnI7Z92esDV0Eb+ZV+PPjE+kPHDhAe7xcrzHfatusfmeF/VrqI67lW1mdWrCZZxwP/TIf1KlFMPhPgGECBHKfj0pdEsJ9ageT+b12lqTS0FJnvVQ/tL6zLJWHlrVFZ62Npa2XnOCZtvQ6vVTzDgAA//9aTkCLnAEAAA=="
        }
      },
      {
        "path": "/etc/sysctl.d/10-disable-ipv6.conf",
        "contents": {
          "source": "data:,net.ipv6.conf.all.disable_ipv6%3D1%0Anet.ipv6.conf.default.disable_ipv6%3D1%0Anet.ipv6.conf.enp0s4.disable_ipv6%3D1%0A"
        }
      }
    ]
  },
  "systemd": {
    "units": [
      {
        "contents": "[Unit]\nRequires=network-online.target\nAfter=network-online.target\nBefore=sshd.service\n[Service]\nType=oneshot\nExecCondition=/usr/bin/test ! -f /etc/python3-for-ansible.done\nExecStart=/usr/bin/sed -i '/\\\\[updates\\\\]/,/^\\\\[/ s/^enabled=.*$/enabled=0/' /etc/yum.repos.d/fedora-updates.repo\nExecStart=/usr/bin/rpm-ostree install python3 libselinux-python3\nExecStart=/usr/bin/sed -i '/\\\\[updates\\\\]/,/^\\\\[/ s/^enabled=.*$/enabled=1/' /etc/yum.repos.d/fedora-updates.repo\nExecStart=/usr/bin/sed -i '/^\\\\[updates\\\\]/a exclude=libxcrypt-compat* mpdecimal* python-pip-wheel* python-setuptools-wheel* python-unversioned-command* python3* python3-libs* python3-selinux*' /etc/yum.repos.d/fedora-updates.repo\nExecStartPost=/usr/bin/touch /etc/python3-for-ansible.done\nExecStartPost=/usr/sbin/shutdown -r now\n[Install]\nWantedBy=multi-user.target\n",
        "enabled": true,
        "name": "python3-for-ansible.service"
      }
    ]
  }
}
[p@ansible core]$


[p@ansible core]$ sha512sum node?.ign
df024d903b92b96c5afb4ce3c66f1e0d5a9cdf6caee98c47063d9fd24bb60d115208334b4a733d90af57ddb335303d0bdfe9ef0c60aa4aa94483bf80bf439d2f  node1.ign
33d40b39476600f9015781865553e09c9fe0542ec72297684c1daae259cab08be202acaec6531353c0650a60df704f16a7f4187ec22fa0d6a09064f5109659e5  node2.ign
4999b6850027407a73b6393ab3e0a6cb0d1068b4d8e5498322b321baa5cd52f6b4e995fd60dc2c17894bb0758ea5cfa8dffdecec9f26c0f6d21d8b4a429bf0c2  node3.ign
d129827dc1c737362f9647c5c90e9dd26d63fbf75ffb216b1b948a378bfcfe9c5f720cd053652008a55efcdf73fe5f2f924e03631e29a5d14a438a83d3fe997e  node4.ign




[p@telekom core]$ sudo firewall-cmd --permanent --zone=public --add-port=8000/tcp
success

[p@telekom core]$ python -m http.server
Serving HTTP on 0.0.0.0 port 8000 (http://0.0.0.0:8000/) ...
192.168.1.201 - - [26/May/2022 21:26:45] "GET /core.ign HTTP/1.1" 200 -
^C
Keyboard interrupt received, exiting.
[p@telekom core]$


ForcePaste:
https://store.steampowered.com/app/1264105/Simplode_Suite__Capture_and_Clipboard_Utilities/


sudo coreos-installer install /dev/vda --ignition-hash=sha512-df024d903b92b96c5afb4ce3c66f1e0d5a9cdf6caee98c47063d9fd24bb60d115208334b4a733d90af57ddb335303d0bdfe9ef0c60aa4aa94483bf80bf439d2f --ignition-url=http://192.168.1.210:8000/node1.ign
sudo coreos-installer install /dev/vda --ignition-hash=sha512-33d40b39476600f9015781865553e09c9fe0542ec72297684c1daae259cab08be202acaec6531353c0650a60df704f16a7f4187ec22fa0d6a09064f5109659e5 --ignition-url=http://192.168.1.210:8000/node2.ign
sudo coreos-installer install /dev/vda --ignition-hash=sha512-4999b6850027407a73b6393ab3e0a6cb0d1068b4d8e5498322b321baa5cd52f6b4e995fd60dc2c17894bb0758ea5cfa8dffdecec9f26c0f6d21d8b4a429bf0c2 --ignition-url=http://192.168.1.210:8000/node3.ign
sudo coreos-installer install /dev/vda --ignition-hash=sha512-d129827dc1c737362f9647c5c90e9dd26d63fbf75ffb216b1b948a378bfcfe9c5f720cd053652008a55efcdf73fe5f2f924e03631e29a5d14a438a83d3fe997e --ignition-url=http://192.168.1.210:8000/node4.ign



sudo systemctl poweroff
remove CDROM (delete virtual drive)
boot



Install Kubernetes via kubeadm on a Fedora CoreOS Node:
https://gist.github.com/codello/118ad0562bfd91e90d5914a9e2dbe606




[p@ansible core]$ cat fcos-k8s.sh
# MIT License
#
# Copyright (c) 2020 Kim Wittenburg
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in all
# copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

# This script prepares a Fedora CoreOS node to create or join a Kubernetes cluster
# via kubeadm. The script may be run directly on a node (or for example via SSH).
# At the end the script will trigger a reboot. When the node comes back up it
# can be joined into the cluster (or create a cluster) via the kubeadm too.
#
# When running kubeadm remember to explicitly specify --cri-socket=/var/run/crio/crio.sock
# in order to use the crio container runtime.

# Kubernetes Version
K8S_VERSION=1.24.1
# The CRICTL_VERSION must match the tag name on GitHub.
# See https://github.com/kubernetes-sigs/cri-tools/releases
CRICTL_VERSION=v1.24.1
# cri-o Versions.
# See https://github.com/cri-o/cri-o/releases
CRIO_VERSION=1.22.4
CRIO_REPO_VERSION="${CRIO_VERSION%.*}"

################################################################################

# Set SELinux in permissive mode (effectively disabling it)
# See: https://github.com/kubernetes/website/issues/14457
sudo setenforce 0
sudo sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config

# Enable the Kubernetes RPM repository. The Fedora repos do have variants of
# these tools but in older versions. By using the official repos we make sure
# that we have access to the latest version.
#
# See: https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/
sudo tee /etc/yum.repos.d/kubernetes.repo <<- EOF > /dev/null
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-\$basearch
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
EOF

# Enable cri-o repository.
# See: https://kubernetes.io/docs/setup/production-environment/container-runtimes/#cri-o
sudo curl -fsSL -o /etc/yum.repos.d/devel:kubic:libcontainers:stable.repo https://download.opensuse.org/repositories/devel:kubic:libcontainers:stable/CentOS_7/devel:kubic:libcontainers:stable.repo
sudo curl -fsSL -o /etc/yum.repos.d/devel:kubic:libcontainers:stable:cri-o:$CRIO_REPO_VERSION.repo https://download.opensuse.org/repositories/devel:kubic:libcontainers:stable:cri-o:$CRIO_REPO_VERSION/CentOS_7/devel:kubic:libcontainers:stable:cri-o:$CRIO_REPO_VERSION.repo

################################################################################

# Configure Network Interfaces for use with cri-o
# See: https://kubernetes.io/docs/setup/production-environment/container-runtimes/#cri-o
# See: https://jebpages.com/2019/02/25/installing-kubeadm-on-fedora-coreos/
# We configure systemd to load the required kernel modules on the next boot.
sudo tee /etc/modules-load.d/crio-net.conf << EOF > /dev/null
# Kernel modules required by the cri-o container engine.
overlay
br_netfilter
EOF
sudo tee /etc/sysctl.d/99-kubernetes-cri.conf <<- EOF > /dev/null
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF
sudo sysctl --system > /dev/null

# The following is equivalent to
#   sudo systemctl enable cri-o
#   sudo systemctl enable kubelet
# However because we haven't installed cri-o and the kubelet yet we create the
# symlinks manually so that both services start on the next boot.
sudo ln -s /usr/lib/systemd/system/kubelet.service /etc/systemd/system/multi-user.target.wants/kubelet.service
sudo ln -s /usr/lib/systemd/system/crio.service /etc/systemd/system/multi-user.target.wants/crio.service


################################################################################

# The cri-tools from the kubernetes repo are relatively old. We replace the
# binary with the one from GitHub.
crictl_file=crictl-$CRICTL_VERSION-$(uname -s)-amd64.tar.gz
curl -fsSLO https://github.com/kubernetes-sigs/cri-tools/releases/download/$CRICTL_VERSION/$crictl_file
sudo tar -zxf $crictl_file -C /usr/local/bin
rm -f $crictl_file

sudo rpm-ostree refresh-md

# Install the kubelet and kubeadm binaries
sudo rpm-ostree install \
  kubelet-$K8S_VERSION \
  kubeadm-$K8S_VERSION \
  kubectl-$K8S_VERSION \
  cri-o-$CRIO_VERSION

# Before rebooting we sleep 1s to allow SSH connections to properly terminate.
# When running interactively this is not required and a simple 'systemctl reboot'
# is sufficient.
#{ sleep 1s; sudo systemctl reboot; } &

[p@ansible core]$




[p@ansible core]$ cat fcos-k8s.yaml
---
# Fedora CoreOS Kubernetes setup with kubeadm

- name: Fedora CoreOS Kubernetes setup with kubeadm
  hosts: masters:workers
  gather_facts: true

  tasks:

  - name: Test connection with ping
    ping:

  - name: Copy fcos-k8s.sh
    ansible.builtin.copy:
      src: /home/p/core/fcos-k8s.sh
      dest: /root/fcos-k8s.sh
      owner: root
      group: root
      mode: '0700'

  - name: Execute fcos-k8s.sh
    raw: /root/fcos-k8s.sh
    register: fcos_k8s_output

  - debug: var=fcos_k8s_output.stdout_lines

  - name: Reboot
    reboot:
      reboot_timeout: 600

  - name: Test connection with ping
    ping:


[p@ansible core]$ cat hosts
[masters]
192.168.1.201

[masters:vars]
ansible_user=core
ansible_connection=ssh
ansible_become=yes

[workers]
192.168.1.202
192.168.1.203
192.168.1.204

[workers:vars]
ansible_user=core
ansible_connection=ssh
ansible_become=yes
[p@ansible core]$


[p@ansible core]$ ansible-playbook -i ./hosts fcos-k8s.yaml

PLAY [Fedora CoreOS Kubernetes setup with kubeadm] ***********************************************************************************************************************

TASK [Gathering Facts] ***************************************************************************************************************************************************
ok: [192.168.1.202]
ok: [192.168.1.203]
ok: [192.168.1.204]
ok: [192.168.1.201]

TASK [Test connection with ping] *****************************************************************************************************************************************
ok: [192.168.1.202]
ok: [192.168.1.204]
ok: [192.168.1.203]
ok: [192.168.1.201]

TASK [Copy fcos-k8s.sh] **************************************************************************************************************************************************
changed: [192.168.1.204]
changed: [192.168.1.201]
changed: [192.168.1.203]
changed: [192.168.1.202]

TASK [Execute fcos-k8s.sh] ***********************************************************************************************************************************************
changed: [192.168.1.203]
changed: [192.168.1.204]
changed: [192.168.1.201]
changed: [192.168.1.202]

TASK [debug] *************************************************************************************************************************************************************
ok: [192.168.1.201] => {
    "fcos_k8s_output.stdout_lines": [
        "sysctl: setting key \"net.ipv4.conf.all.rp_filter\": Invalid argument",
        "sysctl: setting key \"net.ipv4.conf.all.accept_source_route\": Invalid argument",
        "sysctl: setting key \"net.ipv4.conf.all.promote_secondaries\": Invalid argument",
        "Enabled rpm-md repositories: fedora-cisco-openh264 fedora-modular updates-modular fedora updates kubernetes devel_kubic_libcontainers_stable devel_kubic_libcontainers_stable_cri-o_1.22 updates-archive",
        "⠁ Updating metadata for 'updates-modular'   0% [░░░░░░░░░░░░░░░░░░░░] (0s) ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠙ Updating metadata for 'updates-modular'  45% [█████████░░░░░░░░░░░] (1s) ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠚ Updating metadata for 'updates-modular'  78% [███████████████░░░░░] (0s) ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠂ Updating metadata for 'updates-modular' 100% [████████████████████] (0s) ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1AUpdating metadata for 'updates-modular'... done",
        "⠁ Updating metadata for 'updates'   0% [░░░░░░░░░░░░░░░░░░░░] (0s) ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠙ Updating metadata for 'updates'  39% [███████░░░░░░░░░░░░░] (1s) ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠂ Updating metadata for 'updates' 100% [████████████████████] (0s) ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1AUpdating metadata for 'updates'... done",
        "⠁ Updating metadata for 'kubernetes'   0% [░░░░░░░░░░░░░░░░░░░░] (0s) ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠙ Updating metadata for 'kubernetes'  95% [███████████████████░] (0s) ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠚ Updating metadata for 'kubernetes' 100% [████████████████████] (0s) ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1AUpdating metadata for 'kubernetes'... done",
        "⠁ Updating metadata for 'devel_kubic_libcontainers_stable'   0% [░░░░░░░░░░░░░░░░░░░░] (0s) ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠙ Updating metadata for 'devel_kubic_libcontainers_stable'  95% [███████████████████░] (0s) ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠚ Updating metadata for 'devel_kubic_libcontainers_stable' 100% [████████████████████] (0s) ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1AUpdating metadata for 'devel_kubic_libcontainers_stable'... done",
        "⠁ Updating metadata for 'devel_kubic_libcontainers_stable_cri-o_1.22'   0% [░░░░░░░░░░░░░░░░░░░░] (0s) ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠙ Updating metadata for 'devel_kubic_libcontainers_stable_cri-o_1.22'  95% [███████████████████░] (0s) ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠚ Updating metadata for 'devel_kubic_libcontainers_stable_cri-o_1.22' 100% [████████████████████] (0s) ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1AUpdating metadata for 'devel_kubic_libcontainers_stable_cri-o_1.22'... done",
        "⠁ Updating metadata for 'updates-archive'   0% [░░░░░░░░░░░░░░░░░░░░] (0s) ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠙ Updating metadata for 'updates-archive'  95% [███████████████████░] (0s) ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠚ Updating metadata for 'updates-archive' 100% [████████████████████] (0s) ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1AUpdating metadata for 'updates-archive'... done",
        "⠁ Importing rpm-md   0% [░░░░░░░░░░░░░░░░░░░░] (0s) ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠂ Importing rpm-md  44% [████████░░░░░░░░░░░░] (0s) ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠒ Importing rpm-md  55% [███████████░░░░░░░░░] (1s) ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠴ Importing rpm-md  66% [█████████████░░░░░░░] (1s) ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠄ Importing rpm-md  77% [███████████████░░░░░] (1s) ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠤ Importing rpm-md  88% [█████████████████░░░] (1s) ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠠ Importing rpm-md 100% [████████████████████] (0s) ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1AImporting rpm-md... done",
        "rpm-md repo 'fedora-cisco-openh264' (cached); generated: 2022-04-07T16:52:38Z solvables: 4",
        "rpm-md repo 'fedora-modular' (cached); generated: 2022-05-04T21:12:01Z solvables: 825",
        "rpm-md repo 'updates-modular'; generated: 2022-05-25T01:27:07Z solvables: 1121",
        "rpm-md repo 'fedora' (cached); generated: 2022-05-04T21:16:11Z solvables: 67992",
        "rpm-md repo 'updates'; generated: 2022-05-30T04:45:29Z solvables: 9296",
        "rpm-md repo 'kubernetes'; generated: (invalid timestamp) solvables: 810",
        "rpm-md repo 'devel_kubic_libcontainers_stable'; generated: 2022-05-27T17:45:57Z solvables: 19",
        "rpm-md repo 'devel_kubic_libcontainers_stable_cri-o_1.22'; generated: 2022-05-20T09:23:25Z solvables: 9",
        "rpm-md repo 'updates-archive'; generated: 2022-05-30T05:11:40Z solvables: 10469",
        "⠁  ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠉ Checking out tree 096cc2b... ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠙ Checking out tree 096cc2b... ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠚ Checking out tree 096cc2b... ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠒ Checking out tree 096cc2b... ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠂ Checking out tree 096cc2b... ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠂ Checking out tree 096cc2b... ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠒ Checking out tree 096cc2b... ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1AChecking out tree 096cc2b... done",
        "Enabled rpm-md repositories: fedora-cisco-openh264 fedora-modular updates-modular fedora updates kubernetes devel_kubic_libcontainers_stable devel_kubic_libcontainers_stable_cri-o_1.22 updates-archive",
        "⠁ Importing rpm-md   0% [░░░░░░░░░░░░░░░░░░░░] (0s) ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠂ Importing rpm-md  44% [████████░░░░░░░░░░░░] (0s) ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠒ Importing rpm-md  55% [███████████░░░░░░░░░] (1s) ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠠ Importing rpm-md 100% [████████████████████] (0s) ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1AImporting rpm-md... done",
        "rpm-md repo 'fedora-cisco-openh264' (cached); generated: 2022-04-07T16:52:38Z solvables: 4",
        "rpm-md repo 'fedora-modular' (cached); generated: 2022-05-04T21:12:01Z solvables: 825",
        "rpm-md repo 'updates-modular' (cached); generated: 2022-05-25T01:27:07Z solvables: 1132",
        "rpm-md repo 'fedora' (cached); generated: 2022-05-04T21:16:11Z solvables: 67992",
        "rpm-md repo 'updates' (cached); generated: 2022-05-30T04:45:29Z solvables: 10742",
        "rpm-md repo 'kubernetes' (cached); generated: (invalid timestamp) solvables: 810",
        "rpm-md repo 'devel_kubic_libcontainers_stable' (cached); generated: 2022-05-27T17:45:57Z solvables: 19",
        "rpm-md repo 'devel_kubic_libcontainers_stable_cri-o_1.22' (cached); generated: 2022-05-20T09:23:25Z solvables: 9",
        "rpm-md repo 'updates-archive' (cached); generated: 2022-05-30T05:11:40Z solvables: 10469",
        "⠁  ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1AResolving dependencies... done",
        "Will download: 9 packages (91.6 MB)",
        "⠁ Downloading from 'fedora'   0% [░░░░░░░░░░░░░░░░░░░░] (0s) ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠙ Downloading from 'fedora'   4% [░░░░░░░░░░░░░░░░░░░░] (18s) ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠴ Downloading from 'fedora'  23% [████░░░░░░░░░░░░░░░░] (7s) ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠄ Downloading from 'fedora'  31% [██████░░░░░░░░░░░░░░] (6s) ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1ADownloading from 'fedora'... done",
        "⠁ Downloading from 'kubernetes'   0% [░░░░░░░░░░░░░░░░░░░░] (0s) ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠙ Downloading from 'kubernetes'   1% [░░░░░░░░░░░░░░░░░░░░] (43s) ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠤ Downloading from 'kubernetes'   9% [█░░░░░░░░░░░░░░░░░░░] (13s) ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠖ Downloading from 'kubernetes'  17% [███░░░░░░░░░░░░░░░░░] (7s) ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠈ Downloading from 'kubernetes'  25% [█████░░░░░░░░░░░░░░░] (3s) ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠂ Downloading from 'kubernetes'  33% [██████░░░░░░░░░░░░░░] (2s) ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠖ Downloading from 'kubernetes'  46% [█████████░░░░░░░░░░░] (1s) ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠈ Downloading from 'kubernetes'  55% [███████████░░░░░░░░░] (1s) ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠒ Downloading from 'kubernetes'  61% [████████████░░░░░░░░] (1s) ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠲ Downloading from 'kubernetes'  66% [█████████████░░░░░░░] (1s) ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠤ Downloading from 'kubernetes'  69% [█████████████░░░░░░░] (1s) ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠒ Downloading from 'kubernetes'  80% [████████████████░░░░] (1s) ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠂ Downloading from 'kubernetes'  96% [███████████████████░] (0s) ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠴ Downloading from 'kubernetes' 100% [████████████████████] (0s) ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1ADownloading from 'kubernetes'... done",
        "⠁ Downloading from 'devel_kubic_libcontainers_stable_cri-o_1.22'   0% [░░░░░░░░░░░░░░░░░░░░] (0s) ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠙ Downloading from 'devel_kubic_libcontainers_stable_cri-o_1.22'   1% [░░░░░░░░░░░░░░░░░░░░] (1m) ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠂ Downloading from 'devel_kubic_libcontainers_stable_cri-o_1.22'   4% [░░░░░░░░░░░░░░░░░░░░] (42s) ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠤ Downloading from 'devel_kubic_libcontainers_stable_cri-o_1.22'  15% [███░░░░░░░░░░░░░░░░░] (16s) ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠁ Downloading from 'devel_kubic_libcontainers_stable_cri-o_1.22'  28% [█████░░░░░░░░░░░░░░░] (4s) ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠠ Downloading from 'devel_kubic_libcontainers_stable_cri-o_1.22'  42% [████████░░░░░░░░░░░░] (2s) ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠁ Downloading from 'devel_kubic_libcontainers_stable_cri-o_1.22'  56% [███████████░░░░░░░░░] (1s) ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠤ Downloading from 'devel_kubic_libcontainers_stable_cri-o_1.22'  70% [██████████████░░░░░░] (1s) ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠈ Downloading from 'devel_kubic_libcontainers_stable_cri-o_1.22'  84% [████████████████░░░░] (1s) ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠄ Downloading from 'devel_kubic_libcontainers_stable_cri-o_1.22'  97% [███████████████████░] (0s) ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1ADownloading from 'devel_kubic_libcontainers_stable_cri-o_1.22'... done",
        "⠁ Importing packages   0% [░░░░░░░░░░░░░░░░░░░░] (0s) ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠒ Importing packages  33% [██████░░░░░░░░░░░░░░] (0s) ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠂ Importing packages  55% [███████████░░░░░░░░░] (0s) ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠒ Importing packages  66% [█████████████░░░░░░░] (1s) ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠴ Importing packages  88% [█████████████████░░░] (0s) ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠤ Importing packages 100% [████████████████████] (0s) ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1AImporting packages... done",
        "⠁ Checking out packages   0% [░░░░░░░░░░░░░░░░░░░░] (0s) ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠤ Checking out packages  52% [██████████░░░░░░░░░░] (0s) ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1AChecking out packages... done",
        "⠁  ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1ARunning pre scripts... done",
        "⠁  ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠉ Running post scripts... ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠙ Running post scripts... ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1ARunning post scripts... done",
        "⠁  ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠉ Running posttrans scripts... ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠙ Running posttrans scripts... ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠚ Running posttrans scripts... ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠒ Running posttrans scripts... ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠂ Running posttrans scripts... ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠂ Running posttrans scripts... ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠒ Running posttrans scripts... ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠲ Running posttrans scripts... ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠴ Running posttrans scripts... ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠤ Running posttrans scripts... ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1ARunning posttrans scripts... done",
        "⠁  ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠉ Writing rpmdb... ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠙ Writing rpmdb... ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠚ Writing rpmdb... ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠒ Writing rpmdb... ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠂ Writing rpmdb... ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠂ Writing rpmdb... ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠒ Writing rpmdb... ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠲ Writing rpmdb... ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠴ Writing rpmdb... ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠤ Writing rpmdb... ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠄ Writing rpmdb... ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠄ Writing rpmdb... ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠤ Writing rpmdb... ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1AWriting rpmdb... done",
        "⠁  ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠉ Writing OSTree commit... ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠙ Writing OSTree commit... ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠚ Writing OSTree commit... ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠒ Writing OSTree commit... ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠂ Writing OSTree commit... ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠂ Writing OSTree commit... ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠒ Writing OSTree commit... ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠲ Writing OSTree commit... ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠴ Writing OSTree commit... ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠤ Writing OSTree commit... ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠄ Writing OSTree commit... ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠄ Writing OSTree commit... ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠤ Writing OSTree commit... ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1AWriting OSTree commit... done",
        "⠁  ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠉ Staging deployment... ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠙ Staging deployment... ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠚ Staging deployment... ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠒ Staging deployment... ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠂ Staging deployment... ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠂ Staging deployment... ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠒ Staging deployment... ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠲ Staging deployment... ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠴ Staging deployment... ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠤ Staging deployment... ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠄ Staging deployment... ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠄ Staging deployment... ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠤ Staging deployment... ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠠ Staging deployment... ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠠ Staging deployment... ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠤ Staging deployment... ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1A⠦ Staging deployment... ",
        "\u001b[1A",
        "\u001b[2K\u001b[1B\u001b[1AStaging deployment... done",
        "Added:",
        "  conntrack-tools-1.4.6-2.fc36.x86_64",
        "  cri-o-1.22.4-4.2.el7.x86_64",
        "  cri-tools-1.24.0-0.x86_64",
        "  kubeadm-1.24.1-0.x86_64",
        "  kubectl-1.24.1-0.x86_64",
        "  kubelet-1.24.1-0.x86_64",
        "  libnetfilter_cthelper-1.0.0-21.fc36.x86_64",
        "  libnetfilter_cttimeout-1.0.0-19.fc36.x86_64",
        "  libnetfilter_queue-1.0.5-2.fc36.x86_64",
        "Changes queued for next boot. Run \"systemctl reboot\" to start a reboot"
    ]
}

...
...

TASK [Reboot] ************************************************************************************************************************************************************
changed: [192.168.1.202]
changed: [192.168.1.203]
changed: [192.168.1.204]
changed: [192.168.1.201]

TASK [Test connection with ping] *****************************************************************************************************************************************
ok: [192.168.1.202]
ok: [192.168.1.203]
ok: [192.168.1.204]
ok: [192.168.1.201]

PLAY RECAP ***************************************************************************************************************************************************************
192.168.1.201              : ok=7    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
192.168.1.202              : ok=7    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
192.168.1.203              : ok=7    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
192.168.1.204              : ok=7    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0




[p@ansible fcos-k8s]$ ansible-playbook -i ./hosts remove-python3-for-ansible-service.yaml

PLAY [Fedora CoreOS remove python3-for-ansible.service] ***************************************************************

TASK [Gathering Facts] ************************************************************************************************
ok: [192.168.1.203]
ok: [192.168.1.204]
ok: [192.168.1.202]
ok: [192.168.1.201]

TASK [Test connection with ping] **************************************************************************************
ok: [192.168.1.203]
ok: [192.168.1.202]
ok: [192.168.1.204]
ok: [192.168.1.201]

TASK [Disable python3-for-ansible.service] ****************************************************************************
changed: [192.168.1.203]
changed: [192.168.1.202]
changed: [192.168.1.201]
changed: [192.168.1.204]

TASK [Remove file /etc/python3-for-ansible.done] **********************************************************************
changed: [192.168.1.203]
changed: [192.168.1.202]
changed: [192.168.1.201]
changed: [192.168.1.204]

TASK [Remove file /etc/systemd/system/python3-for-ansible.service] ****************************************************
changed: [192.168.1.202]
changed: [192.168.1.201]
changed: [192.168.1.203]
changed: [192.168.1.204]

TASK [Force systemd to reread configs] ********************************************************************************
ok: [192.168.1.203]
ok: [192.168.1.201]
ok: [192.168.1.202]
ok: [192.168.1.204]

TASK [Reboot] *********************************************************************************************************
changed: [192.168.1.201]
changed: [192.168.1.202]
changed: [192.168.1.203]
changed: [192.168.1.204]

TASK [Test connection with ping] **************************************************************************************
ok: [192.168.1.201]
ok: [192.168.1.202]
ok: [192.168.1.204]
ok: [192.168.1.203]

PLAY RECAP ************************************************************************************************************
192.168.1.201              : ok=8    changed=4    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
192.168.1.202              : ok=8    changed=4    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
192.168.1.203              : ok=8    changed=4    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
192.168.1.204              : ok=8    changed=4    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0

[p@ansible fcos-k8s]$





https://www.marcolancini.it/2021/blog-kubernetes-lab-baremetal/


[root@cluster core]$ cat <<EOF > clusterconfig.yml
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
kubernetesVersion: v1.24.1
controllerManager:
  extraArgs:
    flex-volume-plugin-dir: "/etc/kubernetes/kubelet-plugins/volume/exec"
networking:
  podSubnet: 10.244.0.0/16
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
nodeRegistration:
  criSocket: /var/run/crio/crio.sock
EOF


[root@node1 ~]# cat clusterconfig.yml
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
kubernetesVersion: v1.24.1
controllerManager:
  extraArgs:
    flex-volume-plugin-dir: "/etc/kubernetes/kubelet-plugins/volume/exec"
networking:
  podSubnet: 10.244.0.0/16
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
nodeRegistration:
  criSocket: /var/run/crio/crio.sock
[root@node1 ~]# kubeadm init --config clusterconfig.yml
W0530 13:07:05.653502    3204 initconfiguration.go:120] Usage of CRI endpoints without URL scheme is deprecated and can cause kubelet errors in the future. Automatically prepending scheme "unix" to the "criSocket" with value "/var/run/crio/crio.sock". Please update your configuration!
[init] Using Kubernetes version: v1.24.1
[preflight] Running pre-flight checks
        [WARNING SystemVerification]: missing optional cgroups: blkio
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local node1.k8s.local] and IPs [10.96.0.1 192.168.1.201]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [localhost node1.k8s.local] and IPs [192.168.1.201 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [localhost node1.k8s.local] and IPs [192.168.1.201 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[apiclient] All control plane components are healthy after 21.006057 seconds
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node node1.k8s.local as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node node1.k8s.local as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule node-role.kubernetes.io/control-plane:NoSchedule]
[bootstrap-token] Using token: b0n35n.ypc22pqnfhw60r9f
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.1.201:6443 --token b0n35n.ypc22pqnfhw60r9f \
        --discovery-token-ca-cert-hash sha256:e06939370cda901ecd4324fe061b78867576020d5b8fd503e1c48e93c7e0cf05
[root@node1 ~]#




kubeadm join 192.168.1.201:6443 --token b0n35n.ypc22pqnfhw60r9f --discovery-token-ca-cert-hash sha256:e06939370cda901ecd4324fe061b78867576020d5b8fd503e1c48e93c7e0cf05




mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config



[core@telekom ~]$ kubectl get no telekom.ip
NAME         STATUS   ROLES           AGE     VERSION
telekom.ip   Ready    control-plane   4m12s   v1.24.1
[core@telekom ~]$ kubectl get no telekom.ip -oyaml
apiVersion: v1
kind: Node
metadata:
  annotations:
    kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/crio/crio.sock
    node.alpha.kubernetes.io/ttl: "0"
    volumes.kubernetes.io/controller-managed-attach-detach: "true"
  creationTimestamp: "2022-05-26T22:21:38Z"
  labels:
    beta.kubernetes.io/arch: amd64
    beta.kubernetes.io/os: linux
    kubernetes.io/arch: amd64
    kubernetes.io/hostname: telekom.ip
    kubernetes.io/os: linux
    node-role.kubernetes.io/control-plane: ""
    node.kubernetes.io/exclude-from-external-load-balancers: ""
  name: telekom.ip
  resourceVersion: "338"
  uid: f1e83e1e-1a76-4c49-8226-fc27a4cfe5ea
spec:
  podCIDR: 10.244.0.0/24
  podCIDRs:
  - 10.244.0.0/24
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
  - effect: NoSchedule
    key: node-role.kubernetes.io/control-plane
status:
  addresses:
  - address: 192.168.1.201
    type: InternalIP
  - address: telekom.ip
    type: Hostname
  allocatable:
    cpu: "8"
    ephemeral-storage: "28497446046"
    hugepages-1Gi: "0"
    hugepages-2Mi: "0"
    memory: 20379928Ki
    pods: "110"
  capacity:
    cpu: "8"
    ephemeral-storage: 30921708Ki
    hugepages-1Gi: "0"
    hugepages-2Mi: "0"
    memory: 20482328Ki
    pods: "110"
  conditions:
  - lastHeartbeatTime: "2022-05-26T22:21:41Z"
    lastTransitionTime: "2022-05-26T22:21:36Z"
    message: kubelet has sufficient memory available
    reason: KubeletHasSufficientMemory
    status: "False"
    type: MemoryPressure
  - lastHeartbeatTime: "2022-05-26T22:21:41Z"
    lastTransitionTime: "2022-05-26T22:21:36Z"
    message: kubelet has no disk pressure
    reason: KubeletHasNoDiskPressure
    status: "False"
    type: DiskPressure
  - lastHeartbeatTime: "2022-05-26T22:21:41Z"
    lastTransitionTime: "2022-05-26T22:21:36Z"
    message: kubelet has sufficient PID available
    reason: KubeletHasSufficientPID
    status: "False"
    type: PIDPressure
  - lastHeartbeatTime: "2022-05-26T22:21:41Z"
    lastTransitionTime: "2022-05-26T22:21:41Z"
    message: kubelet is posting ready status
    reason: KubeletReady
    status: "True"
    type: Ready
  daemonEndpoints:
    kubeletEndpoint:
      Port: 10250
  images:
  - names:
    - k8s.gcr.io/etcd@sha256:13f53ed1d91e2e11aac476ee9a0269fdda6cc4874eba903efd40daf50c55eee5
    - k8s.gcr.io/etcd@sha256:678382ed340f6996ad40cdba4a4745a2ada41ed9c322c026a2a695338a93dcbe
    - k8s.gcr.io/etcd:3.5.3-0
    sizeBytes: 300857875
  - names:
    - k8s.gcr.io/kube-apiserver@sha256:16b89530529faafa8b49efd7346d3db37e63e151f916c15858966a1f9fb7d869
    - k8s.gcr.io/kube-apiserver@sha256:ad9608e8a9d758f966b6ca6795b50a4723982328194bde214804b21efd48da44
    - k8s.gcr.io/kube-apiserver:v1.24.1
    sizeBytes: 131052788
  - names:
    - k8s.gcr.io/kube-controller-manager@sha256:594a3f5bbdd0419ac57d580da8dfb061237fa48d0c9909991a3af70630291f7a
    - k8s.gcr.io/kube-controller-manager@sha256:f395135baf20b78a8b9cbdc8000c00351ecc2cb6b2ee3b46e0493c85e7e5c196
    - k8s.gcr.io/kube-controller-manager:v1.24.1
    sizeBytes: 120698458
  - names:
    - k8s.gcr.io/kube-proxy@sha256:1652df3138207570f52ae0be05cbf26c02648e6a4c30ced3f779fe3d6295ad6d
    - k8s.gcr.io/kube-proxy@sha256:418ba67e19c8dbd2073cd91223ed8dcffd8a8f185495c2948c5ea16d3e52ab0b
    - k8s.gcr.io/kube-proxy:v1.24.1
    sizeBytes: 111849819
  - names:
    - k8s.gcr.io/kube-scheduler@sha256:0d2de567157e3fb97dfa831620a3dc38d24b05bd3721763a99f3f73b8cbe99c9
    - k8s.gcr.io/kube-scheduler@sha256:c2be69126204c8518eb739f000cbf9c5b1e538a6aac9434e6796df48b82351fb
    - k8s.gcr.io/kube-scheduler:v1.24.1
    sizeBytes: 52335706
  - names:
    - k8s.gcr.io/coredns/coredns@sha256:5b6ec0d6de9baaf3e92d0f66cd96a25b9edbce8716f5f15dcd1a616b3abd590e
    - k8s.gcr.io/coredns/coredns@sha256:8916c89e1538ea3941b58847e448a2c6d940c01b8e716b20423d2d8b189d3972
    - k8s.gcr.io/coredns/coredns:v1.8.6
    sizeBytes: 46959895
  - names:
    - k8s.gcr.io/pause@sha256:bb6ed397957e9ca7c65ada0db5c5d1c707c9c8afc80a94acbe69f3ae76988f0c
    - k8s.gcr.io/pause@sha256:f81611a21cf91214c1ea751c5b525931a0e2ebabe62b3937b6158039ff6f922d
    - k8s.gcr.io/pause:3.7
    sizeBytes: 717997
  - names:
    - k8s.gcr.io/pause@sha256:1ff6c18fbef2045af6b9c16bf034cc421a29027b800e4f9b68ae9b1cb3e9ae07
    - k8s.gcr.io/pause@sha256:369201a612f7b2b585a8e6ca99f77a36bcdbd032463d815388a96800b63ef2c8
    - k8s.gcr.io/pause:3.5
    sizeBytes: 689969
  nodeInfo:
    architecture: amd64
    bootID: 6a1755f1-913f-4e5a-ac69-df3e6d52859c
    containerRuntimeVersion: cri-o://1.22.4
    kernelVersion: 5.17.5-300.fc36.x86_64
    kubeProxyVersion: v1.24.1
    kubeletVersion: v1.24.1
    machineID: 2aba47563fb04da7a35f064e3b434fd5
    operatingSystem: linux
    osImage: Fedora CoreOS 36.20220505.3.2
    systemUUID: bb9b29be-a41a-5735-bf1a-be3faae82285
[core@telekom ~]$



  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
  - effect: NoSchedule
    key: node-role.kubernetes.io/control-plane


kubectl taint nodes --all node-role.kubernetes.io/master-

kubectl taint nodes --all node-role.kubernetes.io/control-plane-


[p@ansible core]$ ansible all -i ./hosts -m raw -a "sysctl net.bridge.bridge-nf-call-iptables=1"
192.168.1.201 | CHANGED | rc=0 >>
net.bridge.bridge-nf-call-iptables = 1
Shared connection to 192.168.1.201 closed.

192.168.1.202 | CHANGED | rc=0 >>
net.bridge.bridge-nf-call-iptables = 1
Shared connection to 192.168.1.202 closed.

192.168.1.203 | CHANGED | rc=0 >>
net.bridge.bridge-nf-call-iptables = 1
Shared connection to 192.168.1.203 closed.

192.168.1.204 | CHANGED | rc=0 >>
net.bridge.bridge-nf-call-iptables = 1
Shared connection to 192.168.1.204 closed.



[core@node1 ~]$ kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Warning: policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
podsecuritypolicy.policy/psp.flannel.unprivileged created
clusterrole.rbac.authorization.k8s.io/flannel created
clusterrolebinding.rbac.authorization.k8s.io/flannel created
serviceaccount/flannel created
configmap/kube-flannel-cfg created
daemonset.apps/kube-flannel-ds created

[core@node1 ~]$ kubectl get po --all-namespaces -owide
NAMESPACE     NAME                                      READY   STATUS    RESTARTS   AGE   IP              NODE              NOMINATED NODE   READINESS GATES
kube-system   coredns-6d4b75cb6d-g6xl7                  1/1     Running   0          49m   10.85.0.3       node1.k8s.local   <none>           <none>
kube-system   coredns-6d4b75cb6d-nf6sq                  1/1     Running   0          49m   10.85.0.2       node1.k8s.local   <none>           <none>
kube-system   etcd-node1.k8s.local                      1/1     Running   0          49m   192.168.1.201   node1.k8s.local   <none>           <none>
kube-system   kube-apiserver-node1.k8s.local            1/1     Running   0          49m   192.168.1.201   node1.k8s.local   <none>           <none>
kube-system   kube-controller-manager-node1.k8s.local   1/1     Running   0          49m   192.168.1.201   node1.k8s.local   <none>           <none>
kube-system   kube-flannel-ds-4cgm5                     1/1     Running   0          30s   192.168.1.204   node4.k8s.local   <none>           <none>
kube-system   kube-flannel-ds-5dcx5                     1/1     Running   0          30s   192.168.1.202   node2.k8s.local   <none>           <none>
kube-system   kube-flannel-ds-q7s9q                     1/1     Running   0          30s   192.168.1.203   node3.k8s.local   <none>           <none>
kube-system   kube-flannel-ds-zfzh9                     1/1     Running   0          30s   192.168.1.201   node1.k8s.local   <none>           <none>
kube-system   kube-proxy-kpd9g                          1/1     Running   0          37m   192.168.1.204   node4.k8s.local   <none>           <none>
kube-system   kube-proxy-lb8tr                          1/1     Running   0          38m   192.168.1.202   node2.k8s.local   <none>           <none>
kube-system   kube-proxy-xdr7c                          1/1     Running   0          37m   192.168.1.203   node3.k8s.local   <none>           <none>
kube-system   kube-proxy-xpbsc                          1/1     Running   0          49m   192.168.1.201   node1.k8s.local   <none>           <none>
kube-system   kube-scheduler-node1.k8s.local            1/1     Running   0          49m   192.168.1.201   node1.k8s.local   <none>           <none>
[core@node1 ~]$


# kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v0.44.0/deploy/static/provider/baremetal/deploy.yaml
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.2.0/deploy/static/provider/baremetal/deploy.yaml


[core@node1 ~]$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.2.0/deploy/static/provider/baremetal/deploy.yaml
namespace/ingress-nginx created
serviceaccount/ingress-nginx created
serviceaccount/ingress-nginx-admission created
role.rbac.authorization.k8s.io/ingress-nginx created
role.rbac.authorization.k8s.io/ingress-nginx-admission created
clusterrole.rbac.authorization.k8s.io/ingress-nginx created
clusterrole.rbac.authorization.k8s.io/ingress-nginx-admission created
rolebinding.rbac.authorization.k8s.io/ingress-nginx created
rolebinding.rbac.authorization.k8s.io/ingress-nginx-admission created
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx created
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx-admission created
configmap/ingress-nginx-controller created
service/ingress-nginx-controller created
service/ingress-nginx-controller-admission created
deployment.apps/ingress-nginx-controller created
job.batch/ingress-nginx-admission-create created
job.batch/ingress-nginx-admission-patch created
ingressclass.networking.k8s.io/nginx created
validatingwebhookconfiguration.admissionregistration.k8s.io/ingress-nginx-admission created

[core@node1 ~]$ kubectl get po --all-namespaces -owide
NAMESPACE       NAME                                        READY   STATUS      RESTARTS   AGE    IP              NODE              NOMINATED NODE   READINESS GATES
ingress-nginx   ingress-nginx-admission-create-v6whm        0/1     Completed   0          18s    10.244.1.2      node2.k8s.local   <none>           <none>
ingress-nginx   ingress-nginx-admission-patch-6gp9t         0/1     Completed   0          18s    10.244.3.2      node4.k8s.local   <none>           <none>
ingress-nginx   ingress-nginx-controller-6b864cf6dd-8q4zh   0/1     Running     0          18s    10.244.2.2      node3.k8s.local   <none>           <none>
kube-system     coredns-6d4b75cb6d-g6xl7                    1/1     Running     0          52m    10.85.0.3       node1.k8s.local   <none>           <none>
kube-system     coredns-6d4b75cb6d-nf6sq                    1/1     Running     0          52m    10.85.0.2       node1.k8s.local   <none>           <none>
kube-system     etcd-node1.k8s.local                        1/1     Running     0          52m    192.168.1.201   node1.k8s.local   <none>           <none>
kube-system     kube-apiserver-node1.k8s.local              1/1     Running     0          52m    192.168.1.201   node1.k8s.local   <none>           <none>
kube-system     kube-controller-manager-node1.k8s.local     1/1     Running     0          52m    192.168.1.201   node1.k8s.local   <none>           <none>
kube-system     kube-flannel-ds-4cgm5                       1/1     Running     0          3m1s   192.168.1.204   node4.k8s.local   <none>           <none>
kube-system     kube-flannel-ds-5dcx5                       1/1     Running     0          3m1s   192.168.1.202   node2.k8s.local   <none>           <none>
kube-system     kube-flannel-ds-q7s9q                       1/1     Running     0          3m1s   192.168.1.203   node3.k8s.local   <none>           <none>
kube-system     kube-flannel-ds-zfzh9                       1/1     Running     0          3m1s   192.168.1.201   node1.k8s.local   <none>           <none>
kube-system     kube-proxy-kpd9g                            1/1     Running     0          39m    192.168.1.204   node4.k8s.local   <none>           <none>
kube-system     kube-proxy-lb8tr                            1/1     Running     0          40m    192.168.1.202   node2.k8s.local   <none>           <none>
kube-system     kube-proxy-xdr7c                            1/1     Running     0          40m    192.168.1.203   node3.k8s.local   <none>           <none>
kube-system     kube-proxy-xpbsc                            1/1     Running     0          52m    192.168.1.201   node1.k8s.local   <none>           <none>
kube-system     kube-scheduler-node1.k8s.local              1/1     Running     0          52m    192.168.1.201   node1.k8s.local   <none>           <none>


https://stackoverflow.com/questions/61373366/networkplugin-cni-failed-to-set-up-pod-xxxxx-network-failed-to-set-bridge-add


[core@telekom ~]$ sudo -i
[root@telekom ~]# ip link set cni0 down && ip link set flannel.1 down
[root@telekom ~]# ip link delete cni0 && ip link delete flannel.1

[root@telekom ~]# systemctl status -l crio
● crio.service - Container Runtime Interface for OCI (CRI-O)
     Loaded: loaded (/usr/lib/systemd/system/crio.service; enabled; vendor preset: disabled)
     Active: active (running) since Thu 2022-05-26 21:48:20 UTC; 1h 36min ago
       Docs: https://github.com/cri-o/cri-o
   Main PID: 958 (crio)
      Tasks: 18
     Memory: 1.2G
        CPU: 1min 51.969s
     CGroup: /system.slice/crio.service
             └─958 /usr/bin/crio

May 26 23:23:56 telekom.ip crio[958]: time="2022-05-26 23:23:56.172242761Z" level=info msg="Pulling image: k8s.gcr.io/ingress-nginx/controller:v1.2.0@sha256:d8196e3bc1e72547c5dec66d6556c0ff92a23f>
May 26 23:23:56 telekom.ip crio[958]: time="2022-05-26 23:23:56.173504889Z" level=info msg="Trying to access \"k8s.gcr.io/ingress-nginx/controller@sha256:d8196e3bc1e72547c5dec66d6556c0ff92a23f6d0>
May 26 23:23:56 telekom.ip crio[958]: time="2022-05-26 23:23:56.881778363Z" level=info msg="Trying to access \"k8s.gcr.io/ingress-nginx/controller@sha256:d8196e3bc1e72547c5dec66d6556c0ff92a23f6d0>
May 26 23:24:02 telekom.ip crio[958]: time="2022-05-26 23:24:02.818870632Z" level=info msg="Pulled image: k8s.gcr.io/ingress-nginx/controller@sha256:2746adf7d60c782b83f6fa6e1ceb938878b9f5b16c217e>
May 26 23:24:02 telekom.ip crio[958]: time="2022-05-26 23:24:02.819833843Z" level=info msg="Checking image status: k8s.gcr.io/ingress-nginx/controller:v1.2.0@sha256:d8196e3bc1e72547c5dec66d6556c0>
May 26 23:24:02 telekom.ip crio[958]: time="2022-05-26 23:24:02.821830481Z" level=info msg="Image status: &{0xc000432850 map[]}" id=33df9287-4010-4a98-8536-aecf003bc3cb name=/runtime.v1.ImageServ>
May 26 23:24:02 telekom.ip crio[958]: time="2022-05-26 23:24:02.822939247Z" level=info msg="Creating container: ingress-nginx/ingress-nginx-controller-6b864cf6dd-8km7c/controller" id=f18fae00-20a>
May 26 23:24:05 telekom.ip crio[958]: time="2022-05-26 23:24:05.345195145Z" level=info msg="Created container 299c889a460e69310641ef2a85d999e720b048d948c9d2310a7b3d9f2cb123d2: ingress-nginx/ingre>
May 26 23:24:05 telekom.ip crio[958]: time="2022-05-26 23:24:05.345979244Z" level=info msg="Starting container: 299c889a460e69310641ef2a85d999e720b048d948c9d2310a7b3d9f2cb123d2" id=0e72d90d-1fc7->
May 26 23:24:05 telekom.ip crio[958]: time="2022-05-26 23:24:05.354916369Z" level=info msg="Started container" PID=35030 containerID=299c889a460e69310641ef2a85d999e720b048d948c9d2310a7b3d9f2cb123>

[root@telekom ~]# systemctl restart crio && systemctl restart kubelet
[root@telekom ~]# exit
logout
[core@telekom ~]$ kubectl get po --all-namespaces -owide
NAMESPACE       NAME                                        READY   STATUS      RESTARTS       AGE   IP              NODE         NOMINATED NODE   READINESS GATES
ingress-nginx   ingress-nginx-admission-create-r4bzg        0/1     Completed   0              44m   10.244.0.134    telekom.ip   <none>           <none>
ingress-nginx   ingress-nginx-admission-patch-2bcrw         0/1     Completed   2              44m   10.244.0.133    telekom.ip   <none>           <none>
ingress-nginx   ingress-nginx-controller-6b864cf6dd-8km7c   1/1     Running     0              44m   10.244.0.135    telekom.ip   <none>           <none>
kube-system     coredns-6d4b75cb6d-hrfbp                    0/1     Running     1 (115s ago)   63m   10.85.0.2       telekom.ip   <none>           <none>
kube-system     coredns-6d4b75cb6d-wlm84                    0/1     Running     1 (115s ago)   63m   10.85.0.3       telekom.ip   <none>           <none>
kube-system     etcd-telekom.ip                             1/1     Running     0              63m   192.168.1.201   telekom.ip   <none>           <none>
kube-system     kube-apiserver-telekom.ip                   1/1     Running     0              64m   192.168.1.201   telekom.ip   <none>           <none>
kube-system     kube-controller-manager-telekom.ip          1/1     Running     0              63m   192.168.1.201   telekom.ip   <none>           <none>
kube-system     kube-flannel-ds-jnkwc                       1/1     Running     0              52m   192.168.1.201   telekom.ip   <none>           <none>
kube-system     kube-proxy-frfcf                            1/1     Running     0              63m   192.168.1.201   telekom.ip   <none>           <none>
kube-system     kube-scheduler-telekom.ip                   1/1     Running     0              63m   192.168.1.201   telekom.ip   <none>           <none>




kubectl get configmap kube-proxy -n kube-system -o yaml  | sed -e "s/strictARP: false/strictARP: true/" | kubectl apply -f - -n kube-system

[core@node1 ~]$ kubectl get configmap kube-proxy -n kube-system -o yaml  | sed -e "s/strictARP: false/strictARP: true/" | kubectl apply -f - -n kube-system
Warning: resource configmaps/kube-proxy is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.
configmap/kube-proxy configured
[core@node1 ~]$




kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.12.1/manifests/namespace.yaml


kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.12.1/manifests/metallb.yaml


[core@node1 ~]$ kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.12.1/manifests/namespace.yaml
namespace/metallb-system created
[core@node1 ~]$ kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.12.1/manifests/metallb.yaml
Warning: policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
podsecuritypolicy.policy/controller created
podsecuritypolicy.policy/speaker created
serviceaccount/controller created
serviceaccount/speaker created
clusterrole.rbac.authorization.k8s.io/metallb-system:controller created
clusterrole.rbac.authorization.k8s.io/metallb-system:speaker created
role.rbac.authorization.k8s.io/config-watcher created
role.rbac.authorization.k8s.io/pod-lister created
role.rbac.authorization.k8s.io/controller created
clusterrolebinding.rbac.authorization.k8s.io/metallb-system:controller created
clusterrolebinding.rbac.authorization.k8s.io/metallb-system:speaker created
rolebinding.rbac.authorization.k8s.io/config-watcher created
rolebinding.rbac.authorization.k8s.io/pod-lister created
rolebinding.rbac.authorization.k8s.io/controller created
daemonset.apps/speaker created
deployment.apps/controller created
[core@node1 ~]$


[core@node1 ~]$ kubectl get po --all-namespaces -owide
NAMESPACE        NAME                                        READY   STATUS      RESTARTS   AGE     IP              NODE              NOMINATED NODE   READINESS GATES
ingress-nginx    ingress-nginx-admission-create-v6whm        0/1     Completed   0          3m30s   10.244.1.2      node2.k8s.local   <none>           <none>
ingress-nginx    ingress-nginx-admission-patch-6gp9t         0/1     Completed   0          3m30s   10.244.3.2      node4.k8s.local   <none>           <none>
ingress-nginx    ingress-nginx-controller-6b864cf6dd-8q4zh   1/1     Running     0          3m30s   10.244.2.2      node3.k8s.local   <none>           <none>
kube-system      coredns-6d4b75cb6d-g6xl7                    1/1     Running     0          55m     10.85.0.3       node1.k8s.local   <none>           <none>
kube-system      coredns-6d4b75cb6d-nf6sq                    1/1     Running     0          55m     10.85.0.2       node1.k8s.local   <none>           <none>
kube-system      etcd-node1.k8s.local                        1/1     Running     0          55m     192.168.1.201   node1.k8s.local   <none>           <none>
kube-system      kube-apiserver-node1.k8s.local              1/1     Running     0          55m     192.168.1.201   node1.k8s.local   <none>           <none>
kube-system      kube-controller-manager-node1.k8s.local     1/1     Running     0          55m     192.168.1.201   node1.k8s.local   <none>           <none>
kube-system      kube-flannel-ds-4cgm5                       1/1     Running     0          6m13s   192.168.1.204   node4.k8s.local   <none>           <none>
kube-system      kube-flannel-ds-5dcx5                       1/1     Running     0          6m13s   192.168.1.202   node2.k8s.local   <none>           <none>
kube-system      kube-flannel-ds-q7s9q                       1/1     Running     0          6m13s   192.168.1.203   node3.k8s.local   <none>           <none>
kube-system      kube-flannel-ds-zfzh9                       1/1     Running     0          6m13s   192.168.1.201   node1.k8s.local   <none>           <none>
kube-system      kube-proxy-kpd9g                            1/1     Running     0          42m     192.168.1.204   node4.k8s.local   <none>           <none>
kube-system      kube-proxy-lb8tr                            1/1     Running     0          44m     192.168.1.202   node2.k8s.local   <none>           <none>
kube-system      kube-proxy-xdr7c                            1/1     Running     0          43m     192.168.1.203   node3.k8s.local   <none>           <none>
kube-system      kube-proxy-xpbsc                            1/1     Running     0          55m     192.168.1.201   node1.k8s.local   <none>           <none>
kube-system      kube-scheduler-node1.k8s.local              1/1     Running     0          55m     192.168.1.201   node1.k8s.local   <none>           <none>
metallb-system   controller-7476b58756-s59g5                 0/1     Running     0          28s     10.244.1.3      node2.k8s.local   <none>           <none>
metallb-system   speaker-mm9rl                               0/1     Running     0          28s     192.168.1.204   node4.k8s.local   <none>           <none>
metallb-system   speaker-s7cn6                               0/1     Running     0          28s     192.168.1.203   node3.k8s.local   <none>           <none>
metallb-system   speaker-wlmmm                               0/1     Running     0          28s     192.168.1.202   node2.k8s.local   <none>           <none>




kubectl create secret generic -n metallb-system memberlist --from-literal=secretkey="$(openssl rand -base64 128)"

[core@node1 ~]$ kubectl create secret generic -n metallb-system memberlist --from-literal=secretkey="$(openssl rand -base64 128)"
error: failed to create secret secrets "memberlist" already exists



[core@node1 ~]$ cat <<EOF > metallb-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  namespace: metallb-system
  name: config
data:
  config: |
    address-pools:
    - name: default
      protocol: layer2
      addresses:
      - 192.168.1.231-192.168.1.250
EOF
[core@node1 ~]$ kubectl apply -f metallb-config.yaml
configmap/config created
[core@node1 ~]$



[core@node1 ~]$ sudo rpm-ostree install helm
Checking out tree 096cc2b... done
Enabled rpm-md repositories: fedora-cisco-openh264 fedora-modular updates-modular fedora updates kubernetes devel_kubic_libcontainers_stable devel_kubic_libcontainers_stable_cri-o_1.22 updates-archive
Importing rpm-md... done
rpm-md repo 'fedora-cisco-openh264' (cached); generated: 2022-04-07T16:52:38Z solvables: 4
rpm-md repo 'fedora-modular' (cached); generated: 2022-05-04T21:12:01Z solvables: 825
rpm-md repo 'updates-modular' (cached); generated: 2022-05-25T01:27:07Z solvables: 1132
rpm-md repo 'fedora' (cached); generated: 2022-05-04T21:16:11Z solvables: 67992
rpm-md repo 'updates' (cached); generated: 2022-05-30T04:45:29Z solvables: 10742
rpm-md repo 'kubernetes' (cached); generated: (invalid timestamp) solvables: 810
rpm-md repo 'devel_kubic_libcontainers_stable' (cached); generated: 2022-05-27T17:45:57Z solvables: 19
rpm-md repo 'devel_kubic_libcontainers_stable_cri-o_1.22' (cached); generated: 2022-05-20T09:23:25Z solvables: 9
rpm-md repo 'updates-archive' (cached); generated: 2022-05-30T05:11:40Z solvables: 10469
Resolving dependencies... done
Will download: 1 package (10.5?MB)
Downloading from 'fedora'... done
Importing packages... done
Checking out packages... done
Running pre scripts... done
Running post scripts... done
Running posttrans scripts... done
Writing rpmdb... done
Writing OSTree commit... done
Staging deployment... done
Added:
  helm-3.5.4-2.fc35.x86_64
Changes queued for next boot. Run "systemctl reboot" to start a reboot
[core@node1 ~]$ sudo systemctl reboot


[core@node1 ~]$ helm repo add haproxy-ingress https://haproxy-ingress.github.io/charts
"haproxy-ingress" has been added to your repositories


[core@node1 ~]$ helm search repo haproxy-ingress
NAME                            CHART VERSION   APP VERSION     DESCRIPTION
haproxy-ingress/haproxy-ingress 0.13.7          v0.13.7         Ingress controller for HAProxy loadbalancer
[core@node1 ~]$


[core@node1 ~]$ cat <<EOF > haproxy-ingress-values.yaml
controller:
  hostNetwork: true
EOF


helm install haproxy-ingress haproxy-ingress/haproxy-ingress --create-namespace --namespace haproxy --version 0.13.7 -f haproxy-config.yaml


[core@node1 ~]$ helm install haproxy-ingress haproxy-ingress/haproxy-ingress --create-namespace --namespace haproxy --version 0.13.7 -f haproxy-ingress-values.yaml
NAME: haproxy-ingress
LAST DEPLOYED: Mon May 30 14:15:40 2022
NAMESPACE: haproxy
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
HAProxy Ingress has been installed!

HAProxy is exposed as a `LoadBalancer` type service.
It may take a few minutes for the LoadBalancer IP to be available.
You can watch the status by running:

    kubectl --namespace haproxy get services haproxy-ingress -o wide -w
[core@node1 ~]$


[core@node1 ~]$ kubectl get pods --all-namespaces -owide
NAMESPACE        NAME                                        READY   STATUS      RESTARTS   AGE   IP              NODE              NOMINATED NODE   READINESS GATES
haproxy          haproxy-ingress-6fb5b5df89-b6wxv            1/1     Running     0          55s   192.168.1.204   node4.k8s.local   <none>           <none>
ingress-nginx    ingress-nginx-admission-create-v6whm        0/1     Completed   0          16m   10.244.1.2      node2.k8s.local   <none>           <none>
ingress-nginx    ingress-nginx-admission-patch-6gp9t         0/1     Completed   0          16m   10.244.3.2      node4.k8s.local   <none>           <none>
ingress-nginx    ingress-nginx-controller-6b864cf6dd-8q4zh   1/1     Running     0          16m   10.244.2.2      node3.k8s.local   <none>           <none>
kube-system      coredns-6d4b75cb6d-g6xl7                    1/1     Running     1          68m   10.244.0.2      node1.k8s.local   <none>           <none>
kube-system      coredns-6d4b75cb6d-nf6sq                    1/1     Running     1          68m   10.244.0.3      node1.k8s.local   <none>           <none>
kube-system      etcd-node1.k8s.local                        1/1     Running     1          68m   192.168.1.201   node1.k8s.local   <none>           <none>
kube-system      kube-apiserver-node1.k8s.local              1/1     Running     1          68m   192.168.1.201   node1.k8s.local   <none>           <none>
kube-system      kube-controller-manager-node1.k8s.local     1/1     Running     1          68m   192.168.1.201   node1.k8s.local   <none>           <none>
kube-system      kube-flannel-ds-4cgm5                       1/1     Running     0          19m   192.168.1.204   node4.k8s.local   <none>           <none>
kube-system      kube-flannel-ds-5dcx5                       1/1     Running     0          19m   192.168.1.202   node2.k8s.local   <none>           <none>
kube-system      kube-flannel-ds-q7s9q                       1/1     Running     0          19m   192.168.1.203   node3.k8s.local   <none>           <none>
kube-system      kube-flannel-ds-zfzh9                       1/1     Running     1          19m   192.168.1.201   node1.k8s.local   <none>           <none>
kube-system      kube-proxy-kpd9g                            1/1     Running     0          55m   192.168.1.204   node4.k8s.local   <none>           <none>
kube-system      kube-proxy-lb8tr                            1/1     Running     0          57m   192.168.1.202   node2.k8s.local   <none>           <none>
kube-system      kube-proxy-xdr7c                            1/1     Running     0          56m   192.168.1.203   node3.k8s.local   <none>           <none>
kube-system      kube-proxy-xpbsc                            1/1     Running     1          68m   192.168.1.201   node1.k8s.local   <none>           <none>
kube-system      kube-scheduler-node1.k8s.local              1/1     Running     1          68m   192.168.1.201   node1.k8s.local   <none>           <none>
metallb-system   controller-7476b58756-s59g5                 1/1     Running     0          13m   10.244.1.3      node2.k8s.local   <none>           <none>
metallb-system   speaker-mm9rl                               1/1     Running     0          13m   192.168.1.204   node4.k8s.local   <none>           <none>
metallb-system   speaker-s7cn6                               1/1     Running     0          13m   192.168.1.203   node3.k8s.local   <none>           <none>
metallb-system   speaker-wlmmm                               1/1     Running     0          13m   192.168.1.202   node2.k8s.local   <none>           <none>


[core@node1 ~]$ kubectl -n haproxy get services haproxy-ingress -owide
NAME              TYPE           CLUSTER-IP     EXTERNAL-IP     PORT(S)                      AGE   SELECTOR
haproxy-ingress   LoadBalancer   10.109.0.234   192.168.1.231   80:31493/TCP,443:31386/TCP   95s   app.kubernetes.io/instance=haproxy-ingress,app.kubernetes.io/name=haproxy-ingress





cat <<EOF > sample-deployment.yaml
---
apiVersion: v1
kind: Namespace
metadata:
  name: test
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: bookinfo-ingress
  annotations:
spec:
  ingressClassName: bookinfo-ingress
  rules:
  - http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: productpage
            port:
              number: 9080
---
apiVersion: v1
kind: Service
metadata:
  name: productpage
  namespace: test
  labels:
    app: productpage
    service: productpage
spec:
  type: LoadBalancer
  ports:
  - name: http
    port: 80
    targetPort: 9080
  selector:
    app: productpage
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: bookinfo-productpage
  namespace: test
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: productpage-v1
  namespace: test
  labels:
    app: productpage
    version: v1
spec:
  replicas: 1
  selector:
    matchLabels:
      app: productpage
      version: v1
  template:
    metadata:
      labels:
        app: productpage
        version: v1
    spec:
      serviceAccountName: bookinfo-productpage
      containers:
      - name: productpage
        image: docker.io/istio/examples-bookinfo-productpage-v1:1.15.0
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 9080
EOF


[core@node1 ~]$ kubectl apply -f sample-deployment.yaml
namespace/test created
ingress.networking.k8s.io/bookinfo-ingress created
service/productpage created
serviceaccount/bookinfo-productpage created
deployment.apps/productpage-v1 created
[core@node1 ~]$



[core@telekom ~]$ kubectl apply -f sample-deployment.yaml
namespace/test unchanged
error: error validating "sample-deployment.yaml": error validating data: [ValidationError(Ingress.spec.rules[0].http.paths[0].backend): unknown field "serviceName" in io.k8s.api.networking.v1.IngressBackend, ValidationError(Ingress.spec.rules[0].http.paths[0].backend): unknown field "servicePort" in io.k8s.api.networking.v1.IngressBackend, ValidationError(Ingress.spec.rules[0].http.paths[0]): missing required field "pathType" in io.k8s.api.networking.v1.HTTPIngressPath]; if you choose to ignore these errors, turn validation off with --validate=false
[core@telekom ~]$

---
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: bookinfo-ingress
  annotations:
    kubernetes.io/ingress.class: haproxy
spec:
  rules:
  - host: product.192.168.1.151.nip.io  # IP of the NUC
    http:
      paths:
      - path: /
        backend:
          serviceName: productpage
          servicePort: 9080
---

https://kubernetes.io/docs/reference/using-api/deprecation-guide/
https://kubernetes.io/docs/concepts/services-networking/ingress/





[core@node1 ~]$ kubectl get service --all-namespaces -owide
NAMESPACE       NAME                                 TYPE           CLUSTER-IP      EXTERNAL-IP     PORT(S)                      AGE   SELECTOR
default         kubernetes                           ClusterIP      10.96.0.1       <none>          443/TCP                      80m   <none>
haproxy         haproxy-ingress                      LoadBalancer   10.109.0.234    192.168.1.231   80:31493/TCP,443:31386/TCP   13m   app.kubernetes.io/instance=haproxy-ingress,app.kubernetes.io/name=haproxy-ingress
ingress-nginx   ingress-nginx-controller             NodePort       10.97.72.207    <none>          80:31994/TCP,443:31405/TCP   28m   app.kubernetes.io/component=controller,app.kubernetes.io/instance=ingress-nginx,app.kubernetes.io/name=ingress-nginx
ingress-nginx   ingress-nginx-controller-admission   ClusterIP      10.110.10.175   <none>          443/TCP                      28m   app.kubernetes.io/component=controller,app.kubernetes.io/instance=ingress-nginx,app.kubernetes.io/name=ingress-nginx
kube-system     kube-dns                             ClusterIP      10.96.0.10      <none>          53/UDP,53/TCP,9153/TCP       80m   k8s-app=kube-dns
test            productpage                          LoadBalancer   10.99.86.118    192.168.1.232   80:30355/TCP                 44s   app=productpage


[core@node1 ~]$ kubectl get ingress --all-namespaces
NAMESPACE   NAME               CLASS              HOSTS   ADDRESS   PORTS   AGE
default     bookinfo-ingress   bookinfo-ingress   *                 80      112s





C:\Users\p>curl -v http://192.168.1.232
*   Trying 192.168.1.232:80...
* Connected to 192.168.1.232 (192.168.1.232) port 80 (#0)
> GET / HTTP/1.1
> Host: 192.168.1.232
> User-Agent: curl/7.79.1
> Accept: */*
>
* Mark bundle as not supporting multiuse
* HTTP 1.0, assume close after body
< HTTP/1.0 200 OK
< Content-Type: text/html; charset=utf-8
< Content-Length: 1683
< Server: Werkzeug/0.14.1 Python/3.6.8
< Date: Fri, 27 May 2022 14:23:24 GMT
<
<!DOCTYPE html>
<html>
  <head>
    <title>Simple Bookstore App</title>
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">

<!-- Latest compiled and minified CSS -->
<link rel="stylesheet" href="static/bootstrap/css/bootstrap.min.css">

<!-- Optional theme -->
<link rel="stylesheet" href="static/bootstrap/css/bootstrap-theme.min.css">

  </head>
  <body>


<p>
    <h3>Hello! This is a simple bookstore application consisting of three services as shown below</h3>
</p>

<table class="table table-condensed table-bordered table-hover"><tr><th>name</th><td>http://details:9080</td></tr><tr><th>endpoint</th><td>details</td></tr><tr><th>children</th><td><table class="table table-condensed table-bordered table-hover"><tr><th>name</th><th>endpoint</th><th>children</th></tr><tr><td>http://details:9080</td><td>details</td><td></td></tr><tr><td>http://reviews:9080</td><td>reviews</td><td><table class="table table-condensed table-bordered table-hover"><tr><th>name</th><th>endpoint</th><th>children</th></tr><tr><td>http://ratings:9080</td><td>ratings</td><td></td></tr></table></td></tr></table></td></tr></table>

<p>
    <h4>Click on one of the links below to auto generate a request to the backend as a real user or a tester
    </h4>
</p>
<p><a href="/productpage?u=normal">Normal user</a></p>
<p><a href="/productpage?u=test">Test user</a></p>



<!-- Latest compiled and minified JavaScript -->
<script src="static/jquery.min.js"></script>

<!-- Latest compiled and minified JavaScript -->
<script src="static/bootstrap/js/bootstrap.min.js"></script>

  </body>
</html>
* Closing connection 0

C:\Users\p>



